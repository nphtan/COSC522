\documentclass[letter,11pt,titlepage]{article}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{float}
\usepackage{subcaption}
\begin{document}

\title{Who Said That?\\ \normalsize{Identifying Tweet Authors with Machine Learning}\\ \small{COSC522 (Machine Learning) Final Project Report}}
\author{Joseph Teague and Nigel Tan\\ \small{jteague6@vols.utk.edu and ntan1@vols.utk.edu}}
\maketitle

\begin{abstract}

    The ability to identify the author of a written work online is important for a number of reasons. While posts are typically tied to a username, individuals can have more than one account on many services and some individuals or organizations have a social media team to handle online posting for them. Author identification can, among other things, assist with tracking down users who are attempting to evadea ban or the author of a post that violates the policies of a celebrity or organization. In this work, we present an attempt to utilize machine learning to identify the individual author of a post. Using a number of machine learning techniques, we show results that have good sensitivity but poor specificity. We show through this work the difficulty of obtaining a reasonable classifier for this type of work without advanced techniques beyond the scope of a relatively short assignment.

\end{abstract}

\section{Introduction}

There is lots of publicly available work for sentiment analysis of Twitter posts (e.g., \cite{TSA1}, \cite{TSA2}, \cite{TSA3}). More work has been done to isolate interests \cite{TIA1} or location \cite{TLA1} of known Twitter users. However, none of this work focuses on identifying potentially unknown Twitter users. At first glance, this may not seem like a worthwhile endeavour. After all, every Twitter user has a username, which provides some level of reduction of anonyminity without necesarilly compromising a real world identity. However, we believe this work could prove beneficial for a number of reasons, primarily ban circumvention detection and ``team post'' author identification. In the former case, author identification could be used to potentially identify users who have attempted to circumvent a ban via the creation of a new account by comparing unknown posts to those from the known, banned account. In the latter case, an individual writer on a social media team could be identified to, for example, locate the origin of a particularly offensive post.

Machine learning is an obvious technique for attempting author identification. However, there are questions about how well it works (and what methods to use) and what features to select from raw text, given that anything like natural language processing is beyond our capabilities. To this end, we examined several supervised (e.g., kNN and SVM) and unsupervise (e.g. k-Means) methods for machine learning and experimented with the features extracted from the text.

The end result, explained in this document, is a complex examination methodology to address the pros and cons of various features and machine learning methods for author identification.

\section{Approach}

\subsection{The Dataset}

We obtained archives containing a number of Tweets from prolific celebrity Twitter users from Kaggle \cite{TweetSource}. While this archive contains a large number of users, we selected six who we thought would provide a diversity of writing style for the purposes of this examination. Those six are Donald Trump, Hillary Clinton, Richard Dawkins, Neil DeGrasse Tyson, Astronaut Scott Kelly, and Kim Kardashian. The Tweets provided are purely text and have no features extracted, so the dity of identifying features fell to us. The features we selected are:

\begin{itemize}
    \item Repeated punctuation (e.g., ``!!!!!'')
    \item Inclusion of images
    \item @s directed at other users or organizations
    \item Typing in all caps (e.g., ``NO COLLUSION'')
    \item Inclusion of links
    \item Quotations (e.g., "...")
    \item Use of hashtags (e.g. ``\#mancrushmonday'')
    \item Quotes
\end{itemize}

We also, as a comparison, run some tests with highly user-specific features extracted. These typically consist of things that one person is more likely to say than the others. For example, Donald Trump is more likely to say ``make America great again,'' while Scott Kelly is the only selected user who would discuss his time in space. These features target specific words and phrases and do not include any sort of natural language processing. The bulk of our tests do not include these highly-specific features, and the end goal is to develop a user-agnostic approach to author identification.

\subsection{Techniques Used}

Every technique used in class so far has been employed in this work. We use MPP cases 1, 2, and 3, k-Nearest-Neighbors, support vector machines (linear, poly, and sigmoid), and backpropagating neural networks were used for supervised learning methods. For unsupervised methods, k-Means, winner-take-all, and Kohonen maps were employed. We also employed decision trees, which were not used in class.

Each learning method was run multiple times (with a parameter sweep where appropriate) with m-fold cross validation to examin its effectiveness. In addition to total accuracy, we paid close attention to sensitivity and specificity. Computing performance was not measured - in this case, we are more interested in overall accuracy than anything else.

\section{Experiments}

All experiments used for this work were written in Python 3. 

\section{Results}

\section{Discussion}

\section{Appendix}

\clearpage
\bibliography{sources}
\bibliographystyle{ieeetr}

\end{document}
